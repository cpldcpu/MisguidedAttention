// data.js
const allData = {
  "trolley_problem_easy": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "trolley_easy_forced": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "dead_schrodingers_cat": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "river_crossing_simple": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "river_crossing_easy": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "river_crossing_even_simpler": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "monty_hall_inverse": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    }
  },
  "jugs_1_liter": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "jugs_3_liters": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "jugs_4_liters": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    }
  },
  "birthday_problem_easy": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "rope_burning_impossible": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "rope_burning_easy": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "less_confusing_monty_hall": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.6666,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "the_normal_barber": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.66666,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.66666,
      "num_evaluations": 3
    }
  },
  "no_paradox_expected_hanging": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "two_door_problem_exit": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "feathers_or_steel": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "parallelism_50_machines": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "kings_mercy": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    }
  },
  "upside_down_bucket": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.06,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "bridge_torch_impossible": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 1
    },
    "gpt-4.1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 1
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "bridge_torch_easy": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "QwQ-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "knights_knaves_impossible": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    }
  },
  "knights_knaves_easy": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 1
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    }
  },
  "poisoned_hot": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "rabbit_inverted_monty_hall": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "feeding_the_goat": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "cabbage_detective": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "ball_and_bat_modified": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "heavy_feather_modified": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    }
  },
  "conjunction_fallacy_simplified": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "conjunction_fallacy_inverted": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "conjunction_fallacy_impossible": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "linear_growth_modified": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "new_years_monty_hall": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "lonely_rabbit_impossible": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    }
  },
  "not_from_hanoi": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "riddle_tall_when_young": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    }
  },
  "riddle_cant_break": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 1
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "riddle_goes_up_never_returns": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 2
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    }
  },
  "riddle_i_never_shave": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "riddle_two_banks_money": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 1
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    }
  },
  "riddle_four_legs_morning": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.75,
      "num_evaluations": 2
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.8333333333333334,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "riddle_occurs_once_in_a_second": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    }
  },
  "unstoppable_force_var1": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 1
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "unstoppable_force_var2": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 1
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "o4-mini-high-long": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "monty_appliance_game_show": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    }
  },
  "monty_appliance_simpler": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 1
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    }
  },
  "monty_defective_appliance": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.5,
      "num_evaluations": 2
    },
    "grok-3-mini-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.0,
      "num_evaluations": 1
    },
    "llama-4-maverick": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    }
  },
  "monty_three_computers": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 1.0,
      "num_evaluations": 2
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    }
  },
  "monty_four_computers": {
    "gpt-4o-2024-11-20": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "command-a-03-2025": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "claude-opus-4": {
      "average_total_score": 0.3333,
      "num_evaluations": 3
    },
    "claude-sonnet-4": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "deepseek-chat-v3-0324": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "DeepSeek-V3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-preview-05-20:thinking": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-03-25": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-exp-1206": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.0-flash": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.0-pro-exp-02-05": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3n-e4b-it": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemma-3-27b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "glm-z1-32b": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-low": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gemini-2.5-pro-preview-05-06": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4.1-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "gpt-4.1-nano": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "grok-3-beta": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "grok-3-mini-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "grok-3-mini-beta-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "Hermes-3-Llama-3.1-70B": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "jamba-1.6-large": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "llama-3.3-70b-instruct": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "deepseek-r1-distill-llama-70b": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "llama-4-maverick": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "minimax-01": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "mistral-small-3": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "mistral-medium-3": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "mistral-small-3.1-2503": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.5-sonnet-new": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "ll3.3-nemotron-49b-v1-thk": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "o3-mini": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o4-mini-high": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "o4-mini-high-long": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "gpt-4o-20250129": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "olmo-2-0325-32b": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "optimus-alpha": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "phi-4-reasoning-plus": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-nothink": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 0.3333333333333333,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "qwen3-30b-a3b-nothink": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "quasar-alpha": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "qwen-max": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "QwQ-32b": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    },
    "deepseek-r1-0528": {
      "average_total_score": 0.6666666666666666,
      "num_evaluations": 3
    },
    "llama-4-scout": {
      "average_total_score": 0.0,
      "num_evaluations": 3
    },
    "claude-3.7-sonnet": {
      "average_total_score": 0.16666666666666666,
      "num_evaluations": 3
    },
    "claude-3.7-thinking-4k": {
      "average_total_score": 0.5,
      "num_evaluations": 3
    }
  }
};
